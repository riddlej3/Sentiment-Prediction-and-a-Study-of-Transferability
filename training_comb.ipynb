{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "os.getcwd()\n",
    "\n",
    "os.chdir('/home/ubuntu/Notebooks/capstone/data')\n",
    "hotels = pd.read_pickle('hotels.pickle').drop_duplicates()\n",
    "hotels = hotels.drop('reviews.id',axis=1)[hotels['language']=='en']\n",
    "hotels['reviews.rating'] = hotels['reviews.rating'].replace(1.0,2.0).replace(0.0,2.0)\n",
    "hotels = hotels[['reviews.text','reviews.rating']]\n",
    "hotels['reviews.rating'] = hotels['reviews.rating'].astype(int)\n",
    "hotels.columns = ['text','ratings_overall']\n",
    "os.chdir('/home/ubuntu/Notebooks/data/ta')\n",
    "ta_data = pd.read_pickle('ta_data2.pickle').replace(1.0,2.0).replace(0.0,2.0).sample(frac=.5,random_state=3)\n",
    "ta_data.columns = ['index', 'id', 'offering_id', 'text', 'ratings_overall', 'language']\n",
    "ta_data = ta_data[ta_data['language'] == 'en']\n",
    "ta_data = ta_data[['text','ratings_overall']]\n",
    "ta_data['ratings_overall'] = ta_data['ratings_overall'].astype(int)\n",
    "\n",
    "X = pd.concat([ta_data,hotels],axis=0).reset_index().drop('index',axis=1)\n",
    "#shuffle X\n",
    "shuffle_idx = np.random.permutation(X.shape[0]).tolist()\n",
    "X = X.loc[shuffle_idx]\n",
    "\n",
    "idx_2 = X[X['ratings_overall'] == 2].index\n",
    "idx_3 = X[X['ratings_overall'] == 3].index\n",
    "idx_4 = X[X['ratings_overall'] == 4].index\n",
    "idx_5 = X[X['ratings_overall'] == 5].index\n",
    "\n",
    "np.random.seed(10)\n",
    "sample_idx_2 = np.random.choice(idx_2,replace=False,size=18264)\n",
    "sample_idx_3 = np.random.choice(idx_3,replace=False,size=18264)\n",
    "sample_idx_4 = np.random.choice(idx_4,replace=False,size=18264)\n",
    "sample_idx_5 = np.random.choice(idx_5,replace=False,size=18264)\n",
    "\n",
    "X = pd.concat([X.loc[sample_idx_2],\n",
    "           X.loc[sample_idx_3],\n",
    "           X.loc[sample_idx_4],\n",
    "           X.loc[sample_idx_5]],axis=0)\n",
    "\n",
    "y = X['ratings_overall']\n",
    "X = X['text']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134.67560501533072"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.apply(lambda x: len(x.split())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/ubuntu/Notebooks/data')\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',\n",
    "                                                 binary=True,\n",
    "                                                 limit=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(X,y,test_size=.30,shuffle=True)\n",
    "\n",
    "maxlen = 150\n",
    "training_samples = x_train.shape[0]\n",
    "validation_samples = x_valid.shape[0]\n",
    "batch_size = 25\n",
    "embedding_dims = 300\n",
    "epochs = 2\n",
    "embedding_dim = 300\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "\n",
    "token = Tokenizer(char_level=False,lower=True)\n",
    "token.fit_on_texts(X)\n",
    "# tokenize.texts_to_sequences(x_train.iloc[0])\n",
    "x_train_seq = pad_sequences(token.texts_to_sequences(x_train),maxlen=maxlen)\n",
    "x_valid_seq = pad_sequences(token.texts_to_sequences(x_valid),maxlen=maxlen)\n",
    "\n",
    "word_index = token.word_index\n",
    "max_words = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((max_words,embedding_dims))\n",
    "for word,i in word_index.items():\n",
    "    if i < max_words:\n",
    "        try:\n",
    "            embedding_vector = word_vectors.get_vector(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "ohe = OneHotEncoder()\n",
    "y_train_new = ohe.fit_transform(y_train.values.reshape(-1,1))\n",
    "y_valid_new = ohe.fit_transform(y_valid.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "model3.add(Dropout(rate=.2))\n",
    "model3.add(LSTM(batch_size,return_sequences=False))\n",
    "model3.add(Dense(4,activation='softmax'))\n",
    "model3.layers[0].set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 51139 samples, validate on 21917 samples\n",
      "Epoch 1/2\n",
      "51139/51139 [==============================] - 523s 10ms/step - loss: 0.9606 - acc: 0.5671 - val_loss: 0.9106 - val_acc: 0.5972\n",
      "Epoch 2/2\n",
      "51139/51139 [==============================] - 516s 10ms/step - loss: 0.7747 - acc: 0.6686 - val_loss: 0.8756 - val_acc: 0.6035\n"
     ]
    }
   ],
   "source": [
    "model3.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',metrics=['acc'])\n",
    "history = model3.fit(x_train_seq,y_train_new.toarray(),epochs=epochs,\n",
    "                     batch_size=batch_size,\n",
    "                     validation_data=(x_valid_seq,\n",
    "                                      y_valid_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/ubuntu/Notebooks/capstone/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('first_model_combined.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/Notebooks/capstone/models'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = open(\"word_dict_combined.pickle\",\"wb\")\n",
    "pickle.dump(word_index,f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
