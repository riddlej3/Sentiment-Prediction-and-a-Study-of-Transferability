{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping and downloading Inside Airbnb links to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "pd.set_option('max_colwidth', 100)\n",
    "pd.set_option('max_columns',500)\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "\n",
    "cities = ['denver','portland','columbus','chicago','pacific-grove']\n",
    "\n",
    "url = 'http://insideairbnb.com/get-the-data.html'\n",
    "r = requests.get(url)\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Functions\n",
    "- upload_file: upload a new file to aws s3 bucket\n",
    "- get_links: crawls the inside airbnb site and downloads links\n",
    "- create_link_dfs: creates a separate dataframe of links for each dataset\n",
    "- load_df: downloads csv from each link for the listed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name=None)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_links(city_list):\n",
    "    \"\"\"\n",
    "    Description: Finds links in beautiful soup and appends urls to list\n",
    "    params: list of cities for which user wants to download data\n",
    "    output: list of urls\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(r.content, \"lxml\")\n",
    "    links = []\n",
    "    for city in city_list:\n",
    "        table = soup.find('table',{'class':f'table table-hover table-striped {city}'})\n",
    "        for row in table.find_all('a'):\n",
    "            links.append(row['href']) if 'listings' in row['href'] else \\\n",
    "            links.append(row['href']) if 'reviews' in row['href'] else 0\n",
    "    return links\n",
    "\n",
    "def create_link_dfs():\n",
    "    \"\"\"\n",
    "    no params:\n",
    "    purpose: takes inside airbnb links, adds them to pandas, and transforms them \n",
    "    into new dfs split by file type.\n",
    "    output: df (primary), and splits df1,df2,df3,df4\n",
    "    \"\"\"\n",
    "    links = get_links(cities)\n",
    "    df = pd.DataFrame(links,columns = ['link'])\n",
    "    #city_stop_index = df.link.str.slice(46).str.index('/')\n",
    "    df_split = df['link'].str.split('/').apply(pd.Series)\\\n",
    "                         .rename(columns = lambda x: 'link_' + str(x))[['link_5',\n",
    "                                                                        'link_6',\n",
    "                                                                        'link_8']]\n",
    "    df = df.join(df_split).rename(columns ={'link':'link',\n",
    "                                            'link_5':'city',\n",
    "                                            'link_6':'date',\n",
    "                                            'link_8':'file'})\n",
    "    \n",
    "    df1 = df[df['file'] == 'listings.csv.gz'].reset_index().drop('index',axis=1)\n",
    "    df2 = df[df['file'] == 'listings.csv'].reset_index().drop('index',axis=1)\n",
    "    df3 = df[df['file'] == 'reviews.csv.gz'].reset_index().drop('index',axis=1)\n",
    "    df4 = df[df['file'] == 'reviews.csv'].reset_index().drop('index',axis=1)\n",
    "    \n",
    "    return df,df1,df2,df3,df4\n",
    "\n",
    "\n",
    "def load_df(load_count,df_file):\n",
    "    \"\"\"\n",
    "    description: loads csvs and gzips for each df_file type\n",
    "    params: count: how many urls shoudl be loaded and concatenated into DF\n",
    "    params: df_file: which of 4 inside airbnb files need downloading\n",
    "    output: new df, list of url indices that failed to download\n",
    "    \"\"\"\n",
    "    \n",
    "    missed_index = []\n",
    "    columns = ['id','city','country_code','country','review_scores_rating','summary']\n",
    "    df1 = pd.read_csv(df_file.reset_index().drop('index',axis=1).iloc[0][0],compression='gzip')[columns]\n",
    "    df1['scrape_date'] = pd.to_datetime(df_file.reset_index().drop('index',axis=1).iloc[0]['date'])\n",
    "    for i in range(1,load_count):\n",
    "        #print('Test_{}'.format(i),'/ Total_{}'.format(load_count),' {}'.format(df_file.reset_index().drop('index',axis=1).iloc[i]['city']))\n",
    "        df2 = df1.copy()\n",
    "        df = df1.copy()\n",
    "        try:\n",
    "            df1 = pd.read_csv(df_file.reset_index().drop('index',axis=1).iloc[i][0],compression='gzip')[columns]\n",
    "            df1['city'] = df_file.reset_index().drop('index',axis=1).iloc[i]['city']\n",
    "            df1['scrape_date'] = pd.to_datetime(df_file.reset_index().drop('index',axis=1).iloc[i]['date'])\n",
    "            df1 = pd.concat([df,df1],sort=False,ignore_index=True)\n",
    "        except:\n",
    "            df1 = df2.copy()\n",
    "            missed_index.append(i)\n",
    "            #print('Fail_{}'.format(i),'/ Total_{}'.format(load_count))\n",
    "            \n",
    "    df1.to_pickle('/home/ubuntu/Notebooks/data/{}.pickle'.format('df1'))\n",
    "    return df1,missed_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DFs for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (43,61,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "df,df1,df2,df3,df4 = create_link_dfs()\n",
    "\n",
    "num_loads1 = len(df1)\n",
    "num_loads2 = len(df2)\n",
    "num_loads3 = len(df3)\n",
    "num_loads4 = len(df4)\n",
    "\n",
    "listings_gz,list1 = load_df(len(df1.iloc[::3]),df1.iloc[::3])\n",
    "\n",
    "# pd.read_csv(df1.reset_index().drop('index',axis=1).iloc[0][0],compression='gzip')['city']\n",
    "# df1.reset_index().drop('index',axis=1).iloc[1]['city']\n",
    "# pd.read_csv(df1.reset_index().drop('index',axis=1).iloc[0][0],compression='gzip')['city']\n",
    "# df1.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(821327, 7)\n"
     ]
    }
   ],
   "source": [
    "print(listings_gz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133975, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_gz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.riddle.001\n",
      "/Users/ginariddle/Desktop/g.school/data/salary_data.csv\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3') \n",
    "s3.list_buckets()\n",
    "s3.list_buckets()['Buckets']\n",
    "for b in s3.list_buckets()['Buckets']:\n",
    "    print(b['Name'])\n",
    "    \n",
    "\n",
    "response = s3.list_objects_v2(Bucket='b.riddle.001')\n",
    "response['Contents']\n",
    "for obj in response['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "upload_file('/Users/ginariddle/Desktop/g.school/data/salary_data.csv', 'b.riddle.001', object_name=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
