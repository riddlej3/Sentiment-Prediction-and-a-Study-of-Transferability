{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip Advisor Model Training Notebook\n",
    "- 90000 rows in balanced training set, 10000 in validation, roughly 40000 in test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing TA Data and Splitting into Test and Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "os.chdir('/home/ubuntu/Notebooks/capstone2/src')\n",
    "from training_data_cleaning import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functions import embedding_mat\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "os.chdir('/home/ubuntu/Notebooks/data/ta')\n",
    "ta_data = ta_data_cleaning(pd.read_pickle('ta_data2.pickle'))\n",
    "ta_data_train = ta_data.replace(1.0,2.0).replace(0.0,2.0).sample(n=150000,random_state=3)\n",
    "train_idx = ta_data_train.index\n",
    "test_idx = [num for num in ta_data.index if num not in train_idx]\n",
    "\n",
    "ta_data.loc[test_idx].to_pickle('/home/ubuntu/Notebooks/data/ta/ta_test.pickle')\n",
    "X = ta_data_train\n",
    "\n",
    "del ta_data\n",
    "del train_idx\n",
    "del test_idx\n",
    "\n",
    "X,y = balance_df_ta(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "- Import word_vectors\n",
    "- preprocess review data and convert to sequence of integers\n",
    "- pad sequences\n",
    "- build model\n",
    "- train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pretrained word2vec word embedding vectors\n",
    "os.chdir('/home/ubuntu/Notebooks')\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz',\n",
    "                                                 binary=True,\n",
    "                                                 limit=3000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess/tokenize/pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_rev_length = int(X.apply(lambda x: len(x.split())).mean())\n",
    "avg_rev_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_valid,y_train,y_valid = train_test_split(X,y,test_size=.1)\n",
    "avg_rev_length = int(X.apply(lambda x: len(x.split())).mean())\n",
    "avg_rev_length\n",
    "\n",
    "### hyperparams for NN Model\n",
    "maxlen = avg_rev_length\n",
    "training_samples = x_train.shape[0]\n",
    "validation_samples = x_valid.shape[0]\n",
    "batch_size = 25\n",
    "embedding_dims = 300\n",
    "epochs = 2\n",
    "embedding_dim = 300\n",
    "\n",
    "token = Tokenizer(char_level=False,lower=True)\n",
    "token.fit_on_texts(X)\n",
    "\n",
    "# token.texts_to_sequences(x_train)\n",
    "x_train_seq = pad_sequences(token.texts_to_sequences(x_train),maxlen=maxlen)\n",
    "x_valid_seq = pad_sequences(token.texts_to_sequences(x_valid),maxlen=maxlen)\n",
    "\n",
    "word_index = token.word_index\n",
    "max_words = len(word_index)\n",
    "embedding_matrix = embedding_mat(max_words,embedding_dim,word_index,word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder()\n",
    "y_train_new = ohe.fit_transform(y_train.values.reshape(-1,1))\n",
    "y_valid_new = ohe.fit_transform(y_valid.values.reshape(-1,1))\n",
    "\n",
    "# x_valid_seq2 = x_valid_seq.reshape(x_valid_seq.shape[0],x_valid_seq.shape[1],1)\n",
    "# x_train_seq2 = x_train_seq.reshape(x_train_seq.shape[0],x_train_seq.shape[1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "model.add(Dropout(rate=.2))\n",
    "model.add(LSTM(batch_size,return_sequences=False))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "model.layers[0].set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 89025 samples, validate on 9892 samples\n",
      "Epoch 1/2\n",
      "89025/89025 [==============================] - 875s 10ms/step - loss: 0.9026 - acc: 0.5935 - val_loss: 0.8040 - val_acc: 0.6404\n",
      "Epoch 2/2\n",
      "89025/89025 [==============================] - 881s 10ms/step - loss: 0.7146 - acc: 0.6885 - val_loss: 0.7994 - val_acc: 0.6419\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',metrics=['acc'])\n",
    "history = model.fit(x_train_seq,y_train_new.toarray(),epochs=epochs,\n",
    "                     batch_size=batch_size,\n",
    "                     validation_data=(x_valid_seq,\n",
    "                                      y_valid_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/ubuntu/Notebooks/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first_model_ta.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/ubuntu/Notebooks/capstone2/data')\n",
    "f = open(\"word_dict_ta_new.pkl\",\"wb\")\n",
    "pickle.dump(word_index,f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
